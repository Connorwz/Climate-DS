{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy as copy\n",
    "import matplotlib as mpl\n",
    "import netCDF4 as ncd\n",
    "#import torchvision\n",
    "import matplotlib.cm as cm\n",
    "#from torch_lr_finder import LRFinder\n",
    "import copy as copy\n",
    "import multiprocessing as mp\n",
    "from datetime import datetime\n",
    "today = datetime.today()\n",
    "# custom modules\n",
    "np.random.seed(100)\n",
    "from scipy import stats\n",
    "import time as time\n",
    "  # setting random seed\n",
    "def corio(lat):\n",
    "    return  2*(2*np.pi/(24*60*60)) * np.sin(lat*(np.pi/180))\n",
    "import matplotlib.font_manager\n",
    "from cuml.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import gc\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=ncd.Dataset(\"/user/wx2309/code_and_data/Data/training_data_for_SF_hbl_gaps_filled.nc\").variables\n",
    "\n",
    "l0=corio(d['l'][:])\n",
    "b00=d['b0'][:]\n",
    "ustar0=d['ustar'][:]\n",
    "h0=d['h'][:]\n",
    "lat0=d['lat'][:]\n",
    "heat0=d['heat'][:]\n",
    "tx0=d['tx'][:] \n",
    "tx0=np.round(tx0,2)\n",
    "SF0=d['SF'][:] \n",
    "\n",
    "ind1=np.where(np.abs(heat0)<601)[0]\n",
    "ind2=np.where(tx0<1.2)[0]\n",
    "ind3=np.where(h0>29)[0]\n",
    "ind4=np.where(h0<301)[0]\n",
    "\n",
    "ind5=np.intersect1d(ind1,ind2)\n",
    "ind6=np.intersect1d(ind3,ind5)\n",
    "ind7=np.intersect1d(ind4,ind6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train_data(data_load):\n",
    "\n",
    "\n",
    "    ind=np.arange(0,len(data_load),1)    # creates a list of indices to shuffle\n",
    "    ind_shuffle=copy.deepcopy(ind)  # deep copies the indices\n",
    "    np.random.shuffle(ind_shuffle)  # shuffles the array\n",
    "    \n",
    "    l_mean=np.mean(data_load[:,0]); \n",
    "    l_std=np.std(data_load[:,0]); \n",
    "    data_load[:,0]=(data_load[:,0]-l_mean)/l_std    #l\n",
    "    \n",
    "    h_mean=np.mean(data_load[:,1]); \n",
    "    h_std=np.std(data_load[:,1]); \n",
    "    data_load[:,1]=(data_load[:,1]-h_mean)/h_std    #b0\n",
    "    \n",
    "    t_mean=np.mean(data_load[:,2]); \n",
    "    t_std=np.std(data_load[:,2]); \n",
    "    data_load[:,2]=(data_load[:,2]-t_mean)/t_std    #u*\n",
    "    \n",
    "    hb_mean= np.mean(data_load[:,3]); \n",
    "    hb_std=np.std(data_load[:,3]); \n",
    "    data_load[:,3]=(data_load[:,3]-hb_mean)/(hb_std)  #w*\n",
    "\n",
    "    stats=np.array([l_mean, l_std, h_mean, h_std, t_mean, t_std, hb_mean, hb_std])\n",
    "    tr_x=data_load[ind_shuffle,0:4]\n",
    "\n",
    "    log_gsigma = data_load[:,4:]\n",
    "\n",
    "    # Take the logarithm of normalized Kappa(sigma)\n",
    "    for j in range(log_gsigma.shape[0]):\n",
    "        log_gsigma[j,:] = np.log(log_gsigma[j,:]/np.max(log_gsigma[j,:]))\n",
    "\n",
    "    log_gsigma_mean=np.mean(log_gsigma,axis=0)\n",
    "    log_gsigma_std= np.std(log_gsigma,axis=0)\n",
    "\n",
    "    k_points=16 #np.shape(data[:,4:])[1]\n",
    "\n",
    "    # Demean logsigma and normalize it with standard deviation\n",
    "    for k in range(k_points):\n",
    "        log_gsigma[:,k]=(log_gsigma[:,k]-log_gsigma_mean[k])/log_gsigma_std[k]\n",
    "\n",
    "    tr_y=log_gsigma\n",
    "    \n",
    "    return tr_x,tr_y, stats, log_gsigma_mean, log_gsigma_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm1=0; mm2=16  #0; 16\n",
    "data_load_main=np.zeros([len(h0[ind7]),4+mm2-mm1])\n",
    "data_load_main[:,0]=l0[ind7]\n",
    "data_load_main[:,1]=b00[ind7]\n",
    "data_load_main[:,2]=ustar0[ind7]\n",
    "data_load_main[:,3]=h0[ind7]\n",
    "data_load_main[:,4:(mm2-mm1+4)]=SF0[ind7,mm1:mm2]\n",
    "\n",
    "data_load3=copy.deepcopy(data_load_main)\n",
    "\n",
    "tr_x,tr_y, stats, log_gsigma_mean, log_gsigma_std=preprocess_train_data(data_load3)\n",
    "\n",
    "valid_data=np.loadtxt('/user/wx2309/code_and_data/Data/data_testing_4_paper.txt')[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind3=np.where(valid_data[:,3]>29)[0]\n",
    "ind4=np.where(valid_data[:,3]<301)[0]\n",
    "ind=np.intersect1d(ind3,ind4)\n",
    "\n",
    "valid_x=valid_data[ind,0:4]\n",
    "\n",
    "valid_x[:,0]=(valid_x[:,0]-stats[0])/stats[1]\n",
    "valid_x[:,1]=(valid_x[:,1]-stats[2])/stats[3]\n",
    "valid_x[:,2]=(valid_x[:,2]-stats[4])/stats[5]\n",
    "valid_x[:,3]=(valid_x[:,3]-stats[6])/stats[7]\n",
    "valid_y=valid_data[ind,5:]\n",
    "\n",
    "for i in range(len(valid_y)):\n",
    "    valid_y[i,:]=np.log(valid_y[i,:]/np.max(valid_y[i,:]))\n",
    "\n",
    "for i in range(16):\n",
    "    valid_y[:,i]=(valid_y[:,i]-log_gsigma_mean[i])/log_gsigma_mean[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It is found that it's computationally infeasible to build the model without GPU acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 10% of data for hyper-parameter tuning\n",
    "ind_tune = np.arange(0,tr_x.shape[0],1)\n",
    "np.random.shuffle(ind_tune) \n",
    "num_selected = round(len(ind_tune)*0.1)\n",
    "tr_x_tune = tr_x[ind_tune,:][:num_selected,:]\n",
    "tr_y_tune = tr_y[ind_tune,:][:num_selected,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/wx2309/.conda/envs/climate/lib/python3.10/site-packages/cuml/internals/api_decorators.py:344: UserWarning: For reproducible results in Random Forest Classifier or for almost reproducible results in Random Forest Regressor, n_streams=1 is recommended. If n_streams is > 1, results may vary due to stream/thread timing differences, even when random_state is set\n",
      "  return func(**kwargs)\n",
      "/user/wx2309/.conda/envs/climate/lib/python3.10/site-packages/cuml/internals/api_decorators.py:188: UserWarning: To use pickling first train using float32 data to fit the estimator\n",
      "  ret = func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mean squared error over 16 nodes are [1.040788936255219, 1.0390749047895937, 1.0418048063311736, 1.0407787755378353, 1.037595594262037, 1.0364541248521977, 1.037819974088181, 1.0395804168768283, 1.0383850807389532, 1.039563560338855, 1.0424294949523403, 1.042318194879242, 1.0424263311159063, 1.0403031044517683, 1.0399748462481109, 1.0392376093556888]\n"
     ]
    }
   ],
   "source": [
    "# Look at the default setting\n",
    "scores_list = []\n",
    "for i in range(16):\n",
    "    scores = []\n",
    "    kf = KFold()\n",
    "    for tr_ind, te_ind in kf.split(np.arange(tr_x_tune.shape[0])):\n",
    "        gc.collect()\n",
    "\n",
    "        tr_x_tune_1 = tr_x_tune[tr_ind,:]\n",
    "        tr_x_tune_2 = tr_x_tune[te_ind,:]\n",
    "        tr_y_tune_1 = tr_y_tune[tr_ind,:]\n",
    "        tr_y_tune_2 = tr_y_tune[te_ind,:]\n",
    "        rfr = RFR(random_state = 66,accuracy_metric=\"mse\")\n",
    "        rfr.fit(tr_x_tune_1,tr_y_tune_1[:,i])\n",
    "        scores.append(rfr.score(tr_x_tune_2,tr_y_tune_2))\n",
    "\n",
    "        del rfr\n",
    "    scores_list.append(np.mean(scores))  \n",
    "    \n",
    "print(f\"the mean squared error over 16 nodes are {scores_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# From the hyper-tuning_for_RF, the output is \n",
    "hyper_df = pd.DataFrame( {'1 column': {'max_depth': 1, 'max_features': 3, 'n_estimators': 130}, \n",
    "                        '2 column': {'max_depth': 2, 'max_features': 1, 'n_estimators': 100}, \n",
    "                        '3 column': {'max_depth': 1, 'max_features': 1, 'n_estimators': 90}, \n",
    "                        '4 column': {'max_depth': 1, 'max_features': 4, 'n_estimators': 150}, \n",
    "                        '5 column': {'max_depth': 1, 'max_features': 1, 'n_estimators': 60}, \n",
    "                        '6 column': {'max_depth': 1, 'max_features': 1, 'n_estimators': 90}, \n",
    "                        '7 column': {'max_depth': 1, 'max_features': 1, 'n_estimators': 70}, \n",
    "                        '8 column': {'max_depth': 1, 'max_features': 1, 'n_estimators': 110}, \n",
    "                        '9 column': {'max_depth': 2, 'max_features': 1, 'n_estimators': 10}, \n",
    "                        '10 column': {'max_depth': 1, 'max_features': 1, 'n_estimators': 10}, \n",
    "                        '11 column': {'max_depth': 1, 'max_features': 1, 'n_estimators': 40}, \n",
    "                        '12 column': {'max_depth': 1, 'max_features': 1, 'n_estimators': 20}, \n",
    "                        '13 column': {'max_depth': 1, 'max_features': 1, 'n_estimators': 70}, \n",
    "                        '14 column': {'max_depth': 1, 'max_features': 1, 'n_estimators': 60}, \n",
    "                        '15 column': {'max_depth': 1, 'max_features': 1, 'n_estimators': 150},\n",
    "                        '16 column': {'max_depth': 1, 'max_features': 1, 'n_estimators': 150}})\n",
    "hyper_score_list = [1.004898988970417, 1.005664965347105, 1.0078069400648908, 1.0107348054347667, \n",
    "                    1.0030113798044062, 0.9991510372487049, 0.9975427637758436, 0.9925515306370241,\n",
    "                    0.9894305826256865, 0.9862748933787826, 0.983556185376802, 0.9963159572151905, \n",
    "                    1.0033396774325605, 1.0006888161916057, 0.9988668042053632, 0.9988153059455639]\n",
    "\n",
    "\n",
    "base_winning_list = [ind for ind, _ in enumerate(list(np.array(scores_list) < np.array(hyper_score_list))) if _]\n",
    "print(base_winning_list)\n",
    "# From the ouput, it's easy to find that in none of nodes that default estimators outperform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              1 column  2 column  3 column  4 column  5 column  6 column  \\\n",
      "max_depth            1         2         1         1         1         1   \n",
      "max_features         3         1         1         4         1         1   \n",
      "n_estimators       130       100        90       150        60        90   \n",
      "\n",
      "              7 column  8 column  9 column  10 column  11 column  12 column  \\\n",
      "max_depth            1         1         2          1          1          1   \n",
      "max_features         1         1         1          1          1          1   \n",
      "n_estimators        70       110        10         10         40         20   \n",
      "\n",
      "              13 column  14 column  15 column  16 column  \n",
      "max_depth             1          1          1          1  \n",
      "max_features          1          1          1          1  \n",
      "n_estimators         70         60        150        150  \n"
     ]
    }
   ],
   "source": [
    "# So the hyper-parameters selected are as follow:\n",
    "hyperparameters = copy.deepcopy(hyper_df)\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_list = [int(num) for num in hyperparameters.iloc[0,:]]\n",
    "max_features_list = [int(num) for num in hyperparameters.iloc[1,:]]\n",
    "n_estimators_list = [int(num) for num in hyperparameters.iloc[2,:]]\n",
    "\n",
    "true_valid_gsigma = copy.deepcopy(valid_data[ind,5:])\n",
    "for i in range(len(true_valid_gsigma)):\n",
    "    true_valid_gsigma[i,:]= true_valid_gsigma[i,:]/np.max(true_valid_gsigma[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/wx2309/.conda/envs/climate/lib/python3.10/site-packages/cuml/internals/api_decorators.py:344: UserWarning: For reproducible results in Random Forest Classifier or for almost reproducible results in Random Forest Regressor, n_streams=1 is recommended. If n_streams is > 1, results may vary due to stream/thread timing differences, even when random_state is set\n",
      "  return func(**kwargs)\n",
      "/user/wx2309/.conda/envs/climate/lib/python3.10/site-packages/cuml/internals/api_decorators.py:188: UserWarning: To use pickling first train using float32 data to fit the estimator\n",
      "  ret = func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "p = np.zeros(16)\n",
    "r2_list = []\n",
    "valid_y_pred = np.zeros((valid_y.shape[0],valid_y.shape[1]))\n",
    "\n",
    "for i in range(16):\n",
    "    max_depth=max_depth_list[i]\n",
    "    max_features=max_features_list[i]\n",
    "    n_estimators=n_estimators_list[i]\n",
    "    rfr = RFR(random_state=66,max_depth=max_depth,max_features=max_features,n_estimators=n_estimators)\n",
    "\n",
    "    rfr.fit(tr_x,tr_y[:,i])\n",
    "    with open(f\"/scratch/wx2309/climate/RF/rfr_{i+1}\", \"wb\") as file:\n",
    "        pickle.dump(rfr,file)\n",
    "\n",
    "    r2_list.append(rfr.score(valid_x,valid_y[:,i]))\n",
    "\n",
    "    valid_y_pred[:,i] = rfr.predict(valid_x)\n",
    "    pred_valid_gsigma =  np.exp(valid_y_pred[:,i] * log_gsigma_std[i] + log_gsigma_mean[i])\n",
    "    asd = pred_valid_gsigma - true_valid_gsigma[:,i]\n",
    "    asd1 = np.percentile(asd,5)\n",
    "    asd2 = np.percentile(asd,95)\n",
    "    ind_iqr1=np.where(asd>asd1)[0]\n",
    "    ind_iqr2=np.where(asd<asd2)[0]\n",
    "    ind_iqr=np.intersect1d(ind_iqr1,ind_iqr2)\n",
    "    p[i] = np.corrcoef(pred_valid_gsigma[ind_iqr], true_valid_gsigma[ind_iqr,i])[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.26083966  0.00419878  0.48304866  0.67056101  0.56627626  0.56336763\n",
      "  0.66973961  0.60930596  0.22907305 -0.09865011  0.09384011  0.69547883\n",
      "  0.69672747  0.67005802  0.55979413  0.5597498 ]\n",
      "the average correlation coefficient is 0.41948309794580607\n"
     ]
    }
   ],
   "source": [
    "print(p)\n",
    "print(f\"the average correlation coefficient is {np.mean(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.06741294892650762, -0.011813668828509538, -0.02948820797713414, -0.05172712707665772, -0.032894707741497387, -0.00638460234868421, -0.0022065738660315137, -0.0005226040460108994, -0.0006072501440990141, -0.0021730988550356933, -0.014290141871515338, -0.05356602790084275, -0.045658233297996675, -0.03231468342969257, -0.025520452733831078, -0.023280812940320805]\n",
      "the average R square is -0.024991321374022935\n"
     ]
    }
   ],
   "source": [
    "print(r2_list)\n",
    "print(f\"the average R square is {np.mean(r2_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
