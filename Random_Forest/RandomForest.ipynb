{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy as copy\n",
    "import matplotlib as mpl\n",
    "import netCDF4 as ncd\n",
    "#import torchvision\n",
    "import matplotlib.cm as cm\n",
    "#from torch_lr_finder import LRFinder\n",
    "import copy as copy\n",
    "import multiprocessing as mp\n",
    "from datetime import datetime\n",
    "today = datetime.today()\n",
    "# custom modules\n",
    "np.random.seed(100)\n",
    "from scipy import stats\n",
    "import time as time\n",
    "  # setting random seed\n",
    "def corio(lat):\n",
    "    return  2*(2*np.pi/(24*60*60)) * np.sin(lat*(np.pi/180))\n",
    "import matplotlib.font_manager\n",
    "from cuml.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import gc\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=ncd.Dataset(\"/user/wx2309/code_and_data/Data/training_data_for_SF_hbl_gaps_filled.nc\").variables\n",
    "\n",
    "l0=corio(d['l'][:])\n",
    "b00=d['b0'][:]\n",
    "ustar0=d['ustar'][:]\n",
    "h0=d['h'][:]\n",
    "lat0=d['lat'][:]\n",
    "heat0=d['heat'][:]\n",
    "tx0=d['tx'][:] \n",
    "tx0=np.round(tx0,2)\n",
    "SF0=d['SF'][:] \n",
    "\n",
    "ind1=np.where(np.abs(heat0)<601)[0]\n",
    "ind2=np.where(tx0<1.2)[0]\n",
    "ind3=np.where(h0>29)[0]\n",
    "ind4=np.where(h0<301)[0]\n",
    "\n",
    "ind5=np.intersect1d(ind1,ind2)\n",
    "ind6=np.intersect1d(ind3,ind5)\n",
    "ind7=np.intersect1d(ind4,ind6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train_data(data_load):\n",
    "\n",
    "\n",
    "    ind=np.arange(0,len(data_load),1)    # creates a list of indices to shuffle\n",
    "    ind_shuffle=copy.deepcopy(ind)  # deep copies the indices\n",
    "    np.random.shuffle(ind_shuffle)  # shuffles the array\n",
    "    \n",
    "    l_mean=np.mean(data_load[:,0]); \n",
    "    l_std=np.std(data_load[:,0]); \n",
    "    data_load[:,0]=(data_load[:,0]-l_mean)/l_std    #l\n",
    "    \n",
    "    h_mean=np.mean(data_load[:,1]); \n",
    "    h_std=np.std(data_load[:,1]); \n",
    "    data_load[:,1]=(data_load[:,1]-h_mean)/h_std    #b0\n",
    "    \n",
    "    t_mean=np.mean(data_load[:,2]); \n",
    "    t_std=np.std(data_load[:,2]); \n",
    "    data_load[:,2]=(data_load[:,2]-t_mean)/t_std    #u*\n",
    "    \n",
    "    hb_mean= np.mean(data_load[:,3]); \n",
    "    hb_std=np.std(data_load[:,3]); \n",
    "    data_load[:,3]=(data_load[:,3]-hb_mean)/(hb_std)  #w*\n",
    "\n",
    "    stats=np.array([l_mean, l_std, h_mean, h_std, t_mean, t_std, hb_mean, hb_std])\n",
    "    tr_x=data_load[ind_shuffle,0:4]\n",
    "\n",
    "    log_gsigma = data_load[:,4:]\n",
    "\n",
    "    # Take the logarithm of normalized Kappa(sigma)\n",
    "    for j in range(log_gsigma.shape[0]):\n",
    "        log_gsigma[j,:] = np.log(log_gsigma[j,:]/np.max(log_gsigma[j,:]))\n",
    "\n",
    "    log_gsigma_mean=np.mean(log_gsigma,axis=0)\n",
    "    log_gsigma_std= np.std(log_gsigma,axis=0)\n",
    "\n",
    "    k_points=16 #np.shape(data[:,4:])[1]\n",
    "\n",
    "    # Demean logsigma and normalize it with standard deviation\n",
    "    for k in range(k_points):\n",
    "        log_gsigma[:,k]=(log_gsigma[:,k]-log_gsigma_mean[k])/log_gsigma_std[k]\n",
    "\n",
    "    tr_y=log_gsigma\n",
    "    \n",
    "    return tr_x,tr_y, stats, log_gsigma_mean, log_gsigma_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm1=0; mm2=16  #0; 16\n",
    "data_load_main=np.zeros([len(h0[ind7]),4+mm2-mm1])\n",
    "data_load_main[:,0]=l0[ind7]\n",
    "data_load_main[:,1]=b00[ind7]\n",
    "data_load_main[:,2]=ustar0[ind7]\n",
    "data_load_main[:,3]=h0[ind7]\n",
    "data_load_main[:,4:(mm2-mm1+4)]=SF0[ind7,mm1:mm2]\n",
    "\n",
    "data_load3=copy.deepcopy(data_load_main)\n",
    "\n",
    "tr_x,tr_y, stats, log_gsigma_mean, log_gsigma_std=preprocess_train_data(data_load3)\n",
    "\n",
    "valid_data=np.loadtxt('/user/wx2309/code_and_data/Data/data_testing_4_paper.txt')[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind3=np.where(valid_data[:,3]>29)[0]\n",
    "ind4=np.where(valid_data[:,3]<301)[0]\n",
    "ind=np.intersect1d(ind3,ind4)\n",
    "\n",
    "valid_x=valid_data[ind,0:4]\n",
    "\n",
    "valid_x[:,0]=(valid_x[:,0]-stats[0])/stats[1]\n",
    "valid_x[:,1]=(valid_x[:,1]-stats[2])/stats[3]\n",
    "valid_x[:,2]=(valid_x[:,2]-stats[4])/stats[5]\n",
    "valid_x[:,3]=(valid_x[:,3]-stats[6])/stats[7]\n",
    "valid_y=valid_data[ind,5:]\n",
    "\n",
    "for i in range(len(valid_y)):\n",
    "    valid_y[i,:]=np.log(valid_y[i,:]/np.max(valid_y[i,:]))\n",
    "\n",
    "for i in range(16):\n",
    "    valid_y[:,i]=(valid_y[:,i]-log_gsigma_mean[i])/log_gsigma_mean[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It is found that it's computationally infeasible to build the model without GPU acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 10% of data for hyper-parameter tuning\n",
    "ind_tune = np.arange(0,tr_x.shape[0],1)\n",
    "np.random.shuffle(ind_tune) \n",
    "num_selected = round(len(ind_tune)*0.1)\n",
    "tr_x_tune = tr_x[ind_tune,:][:num_selected,:]\n",
    "tr_y_tune = tr_y[ind_tune,:][:num_selected,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/wx2309/.conda/envs/climate/lib/python3.10/site-packages/cuml/internals/api_decorators.py:344: UserWarning: For reproducible results in Random Forest Classifier or for almost reproducible results in Random Forest Regressor, n_streams=1 is recommended. If n_streams is > 1, results may vary due to stream/thread timing differences, even when random_state is set\n",
      "  return func(**kwargs)\n",
      "/user/wx2309/.conda/envs/climate/lib/python3.10/site-packages/cuml/internals/api_decorators.py:188: UserWarning: To use pickling first train using float32 data to fit the estimator\n",
      "  ret = func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mean absolute error over 16 nodes are [0.8371911792893234, 0.8364259821442221, 0.8368531286229919, 0.8368062022859423, 0.8359707409936302, 0.8353894924303444, 0.8356293116254664, 0.8360977677501442, 0.8354647271533626, 0.835805181145432, 0.8367595898873794, 0.8374543613746326, 0.8379020805788434, 0.8376159619874285, 0.8375692191734441, 0.8374975942673535]\n"
     ]
    }
   ],
   "source": [
    "# Look at the default setting\n",
    "scores_list = []\n",
    "for i in range(16):\n",
    "    scores = []\n",
    "    kf = KFold()\n",
    "    for tr_ind, te_ind in kf.split(np.arange(tr_x_tune.shape[0])):\n",
    "        gc.collect()\n",
    "\n",
    "        tr_x_tune_1 = tr_x_tune[tr_ind,:]\n",
    "        tr_x_tune_2 = tr_x_tune[te_ind,:]\n",
    "        tr_y_tune_1 = tr_y_tune[tr_ind,:]\n",
    "        tr_y_tune_2 = tr_y_tune[te_ind,:]\n",
    "        rfr = RFR(random_state = 66,accuracy_metric=\"mean_ae\")\n",
    "        rfr.fit(tr_x_tune_1,tr_y_tune_1[:,i])\n",
    "        scores.append(rfr.score(tr_x_tune_2,tr_y_tune_2))\n",
    "\n",
    "        del rfr\n",
    "    scores_list.append(np.mean(scores))  \n",
    "    \n",
    "print(f\"the mean absolute error over 16 nodes are {scores_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n"
     ]
    }
   ],
   "source": [
    "# From the hyper-tuning_for_RF, the output is \n",
    "hyper_df = pd.DataFrame( {'1 column': {'max_depth': 5, 'max_features': 2, 'n_estimators': 50}, \n",
    "                          '2 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 80}, \n",
    "                          '3 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 140}, \n",
    "                          '4 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 20}, \n",
    "                          '5 column': {'max_depth': 8, 'max_features': 2, 'n_estimators': 30}, \n",
    "                          '6 column': {'max_depth': 6, 'max_features': 3, 'n_estimators': 90}, \n",
    "                          '7 column': {'max_depth': 5, 'max_features': 2, 'n_estimators': 90}, \n",
    "                          '8 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 90}, \n",
    "                          '9 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 90}, \n",
    "                          '10 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 10}, \n",
    "                          '11 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 10}, \n",
    "                          '12 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 20}, \n",
    "                          '13 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 30}, \n",
    "                          '14 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 120}, \n",
    "                          '15 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 120}, \n",
    "                          '16 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 140}})\n",
    "hyper_score_list = [0.8261802564920877, 0.7558332389563769, 0.641316259121996, 0.6665849246506932,\n",
    "                    0.900231269164701, 0.8273990549184675, 0.744042427685627, 0.6937356107115706, \n",
    "                    0.6758924643351577, 0.6256811290788289, 0.5977666525426375, 0.7530039657558006, \n",
    "                    0.7810863614024693, 0.7597234201281696, 0.7315923597523083, 0.6940271663365525]\n",
    "\n",
    "base_winning_list = [ind for ind, _ in enumerate(list(np.array(scores_list) < np.array(hyper_score_list))) if _]\n",
    "print(base_winning_list)\n",
    "# From the ouput, it's easy to find that default estimators outperform in 5th column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              1 column  2 column  3 column  4 column  5 column  6 column  \\\n",
      "max_depth            5         5         5         5        16         6   \n",
      "max_features         2         1         1         1         4         3   \n",
      "n_estimators        50        80       140        20       100        90   \n",
      "\n",
      "              7 column  8 column  9 column  10 column  11 column  12 column  \\\n",
      "max_depth            5         5         5          5          5          5   \n",
      "max_features         2         1         1          1          1          1   \n",
      "n_estimators        90        90        90         10         10         20   \n",
      "\n",
      "              13 column  14 column  15 column  16 column  \n",
      "max_depth             5          5          5          5  \n",
      "max_features          1          1          1          1  \n",
      "n_estimators         30        120        120        140  \n"
     ]
    }
   ],
   "source": [
    "# So the hyper-parameters selected are as follow:\n",
    "hyperparameters = pd.DataFrame({'1 column': {'max_depth': 5, 'max_features': 2, 'n_estimators': 50}, \n",
    "                          '2 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 80}, \n",
    "                          '3 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 140}, \n",
    "                          '4 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 20}, \n",
    "                          '5 column': {'max_depth': 16, 'max_features': 4, 'n_estimators': 100}, \n",
    "                          '6 column': {'max_depth': 6, 'max_features': 3, 'n_estimators': 90}, \n",
    "                          '7 column': {'max_depth': 5, 'max_features': 2, 'n_estimators': 90}, \n",
    "                          '8 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 90}, \n",
    "                          '9 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 90}, \n",
    "                          '10 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 10}, \n",
    "                          '11 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 10}, \n",
    "                          '12 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 20}, \n",
    "                          '13 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 30}, \n",
    "                          '14 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 120}, \n",
    "                          '15 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 120}, \n",
    "                          '16 column': {'max_depth': 5, 'max_features': 1, 'n_estimators': 140}})\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_list = [int(num) for num in hyperparameters.iloc[0,:]]\n",
    "max_features_list = [int(num) for num in hyperparameters.iloc[1,:]]\n",
    "n_estimators_list = [int(num) for num in hyperparameters.iloc[2,:]]\n",
    "\n",
    "true_valid_gsigma = copy.deepcopy(valid_data[ind,5:])\n",
    "for i in range(len(true_valid_gsigma)):\n",
    "    true_valid_gsigma[i,:]= true_valid_gsigma[i,:]/np.max(true_valid_gsigma[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the correlation coefficient with absolute value is 0.32150300719187574\n"
     ]
    }
   ],
   "source": [
    "p = np.zeros(16)\n",
    "valid_y_pred = np.zeros((valid_y.shape[0],valid_y.shape[1]))\n",
    "\n",
    "for i in range(16):\n",
    "    max_depth=max_depth_list[i]\n",
    "    max_features=max_features_list[i]\n",
    "    n_estimators=n_estimators_list[i]\n",
    "    rfr = RFR(random_state=66,max_depth=max_depth,max_features=max_features,n_estimators=n_estimators,accuracy_metric = \"mean_ae\")\n",
    "\n",
    "    rfr.fit(tr_x,tr_y[:,i])\n",
    "    with open(f\"/scratch/wx2309/climate/RF/rfr_{i+1}\", \"wb\") as file:\n",
    "        pickle.dump(rfr,file)\n",
    "    valid_y_pred[:,i] = rfr.predict(valid_x)\n",
    "    \n",
    "    pred_valid_gsigma =  np.exp(valid_y_pred[:,i] * log_gsigma_std[i] + log_gsigma_mean[i])\n",
    "    \n",
    "    asd = pred_valid_gsigma - true_valid_gsigma[:,i]\n",
    "    asd1 = np.percentile(asd,5)\n",
    "    asd2 = np.percentile(asd,95)\n",
    "    ind_iqr1=np.where(asd>asd1)[0]\n",
    "    ind_iqr2=np.where(asd<asd2)[0]\n",
    "    ind_iqr=np.intersect1d(ind_iqr1,ind_iqr2)\n",
    "    p[i] = np.corrcoef(pred_valid_gsigma[ind_iqr], true_valid_gsigma[ind_iqr,i])[0,1]\n",
    "print(f\"the correlation coefficient with absolute value is {np.mean(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08134161 -0.04290756  0.38445748  0.45854659  0.10413562  0.35509016\n",
      "  0.47241801  0.51047595  0.34332649 -0.16433938  0.30368587  0.54133026\n",
      "  0.52774066  0.54851907  0.44351016  0.43940034]\n"
     ]
    }
   ],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
